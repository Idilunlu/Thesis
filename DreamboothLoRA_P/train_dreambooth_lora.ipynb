{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "#\n",
    "# This file is adapted from https://github.com/huggingface/diffusers/blob/febaf863026bd014b7a14349336544fc109d0f57/examples/dreambooth/train_dreambooth_lora.py\n",
    "# The original license is as below:\n",
    "#\n",
    "# Copyright 2022 The HuggingFace Inc. team. All rights reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import argparse\n",
    "import hashlib\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "import datasets\n",
    "import diffusers\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    DiffusionPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.cross_attention import LoRACrossAttnProcessor\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.10 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Callable\n",
    "import h5py\n",
    "# from utils.helpers import get_transforms, seed_everything\n",
    "# from utils.dataset import load_success_ids\n",
    "from random import sample\n",
    "from tqdm import tqdm\n",
    "from PIL import Image\n",
    "import sqlite3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def get_transforms(train=False):\n",
    "    \"\"\"\n",
    "    Takes a list of images and applies the same augmentations to all of them.\n",
    "    This is completely overengineered but it makes it easier to use in our pipeline\n",
    "    as drop-in replacement for torchvision transforms.\n",
    "    ## Example\n",
    "    ``` python\n",
    "    imgs = [Image.open(f”image{i}.png”) for i in range(1, 4)]\n",
    "    t = get_albumentations_transforms(train=True)\n",
    "    t_imgs = t(imgs) # List[torch.Tensor]\n",
    "    ```\n",
    "    For the single image case:\n",
    "    ```python\n",
    "    img = Image.open(f”image{0}.png”)\n",
    "    # or img = np.load(some_bytes)\n",
    "    t = get_albumentations_transforms(train=True)\n",
    "    t_img = t(img) # torch.Tensor\n",
    "    ```\n",
    "    \"\"\"\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    _data_transform = None\n",
    "    def _get_transform(n: int = 3):\n",
    "        if train:\n",
    "            data_transforms = A.Compose(\n",
    "                [\n",
    "                    A.Resize(224, 224),\n",
    "                    A.OneOf(\n",
    "                        [\n",
    "                            A.Rotate(limit=0, p=1),\n",
    "                            A.Rotate(limit=90, p=1),\n",
    "                            A.Rotate(limit=180, p=1),\n",
    "                            A.Rotate(limit=270, p=1),\n",
    "                        ],\n",
    "                        p=0.5,\n",
    "                    ),\n",
    "                    A.Compose(\n",
    "                        [\n",
    "                            A.OneOf(\n",
    "                                [\n",
    "                                    A.ColorJitter(\n",
    "                                        brightness=(0.9, 1),\n",
    "                                        contrast=(0.9, 1),\n",
    "                                        saturation=(0.9, 1),\n",
    "                                        hue=(0, 0.1),\n",
    "                                        p=1.0,\n",
    "                                    ),\n",
    "                                    A.Affine(\n",
    "                                        scale=(0.5, 1.5),\n",
    "                                        translate_percent=(0.0, 0.0),\n",
    "                                        shear=(0.5, 1.5),\n",
    "                                        p=1.0,\n",
    "                                    ),\n",
    "                                ],\n",
    "                                p=0.5,\n",
    "                            ),\n",
    "                            A.GaussianBlur(\n",
    "                                blur_limit=(1, 3), sigma_limit=(0.1, 3), p=1.0\n",
    "                            ),\n",
    "                        ]\n",
    "                    ),\n",
    "                    A.OneOf(\n",
    "                        [\n",
    "                            A.HorizontalFlip(p=0.5),\n",
    "                            A.VerticalFlip(p=0.5),\n",
    "                        ],\n",
    "                        p=0.5,\n",
    "                    ),\n",
    "                    A.Normalize(mean=mean, std=std),\n",
    "                    ToTensorV2(),\n",
    "                ],\n",
    "                additional_targets={f\"image{i}\": \"image\" for i in range(1, n)},\n",
    "            )\n",
    "        else:\n",
    "            data_transforms = A.Compose(\n",
    "                [\n",
    "                    A.Resize(224, 224),\n",
    "                    A.Normalize(mean=mean, std=std),\n",
    "                    ToTensorV2(),\n",
    "                ],\n",
    "                additional_targets={f\"image{i}\": \"image\" for i in range(1, n)},\n",
    "\n",
    "            )\n",
    "        return data_transforms\n",
    "    def transform_images(images: any):\n",
    "        nonlocal _data_transform\n",
    "        if not isinstance(images, list):\n",
    "            n = 1\n",
    "            images = [images]\n",
    "        else:\n",
    "            n = len(images)\n",
    "        if _data_transform is None:\n",
    "            # instantiate once\n",
    "            _data_transform = _get_transform(n)\n",
    "        # accepts both lists of np.Array and PIL.Image\n",
    "        if isinstance(images[0], Image.Image):\n",
    "            images = [np.array(img) for img in images]\n",
    "        image_dict = {\"image\": images[0]}\n",
    "        for i in range(1, n):\n",
    "            image_dict[f\"image{i}\"] = images[i]\n",
    "        transformed = _data_transform(**image_dict)\n",
    "        transformed_images = [\n",
    "            transformed[key] for key in transformed.keys() if \"image\" in key\n",
    "        ]\n",
    "        if len(transformed_images) == 1:\n",
    "            return transformed_images[0]\n",
    "        return transformed_images\n",
    "    return transform_images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def load_success_ids(feat_folder: str):\n",
    "    \"\"\"\n",
    "    Backwards-compatible loading of success IDs.\n",
    "    We either load the available slide ids from the deprecated success.txt file\n",
    "    or from the success.db sqlite database.\n",
    "    If both files exist, we always prefer the database.\n",
    "    \"\"\"\n",
    "    success_ids = set()\n",
    "    success_txt = f\"{feat_folder}/success.txt\"\n",
    "    success_db = f\"{feat_folder}/success.db\"\n",
    "    if os.path.exists(success_txt):\n",
    "        print(\"Warning: Loading success IDs from deprecated success.txt.\")\n",
    "        with open(success_txt, \"r\") as f:\n",
    "            for line in f:\n",
    "                success_ids.add(line.strip())\n",
    "    if os.path.exists(success_db):\n",
    "        print(\"Loading success IDs from database.\")\n",
    "        conn = sqlite3.connect(success_db)\n",
    "        cursor = conn.cursor()\n",
    "        cursor.execute(\"SELECT slide_id FROM success\")\n",
    "        success_ids = set([row[0] for row in cursor.fetchall()])\n",
    "        conn.close()\n",
    "    return success_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def seed_everything(seed: int):\n",
    "    torch.manual_seed(seed)\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.manual_seed(seed)\n",
    "        torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 42\n",
    "seed_everything(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class IdilDataSet(Dataset):\n",
    "    \"\"\"\n",
    "    Only for single dataset classes!\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        folder: str,\n",
    "        magnification: int,\n",
    "        transform: Optional[Callable] = get_transforms(),\n",
    "        n_patches: int = 250,\n",
    "        random_selection=False,\n",
    "        limit: Optional[int] = None,\n",
    "        wsi_type: str = \"frozen\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.csv = pd.read_csv(csv_path)\n",
    "        self.csv = self.csv[self.csv[\"wsi_type\"] == wsi_type]\n",
    "\n",
    "        # Filter out unwanted tumor types\n",
    "        self.csv = self.csv[self.csv[\"Tumor Type\"] != \"Oligoastrocytoma\"]\n",
    "        \n",
    "        # Replace grade values\n",
    "        self.csv[\"Neoplasm Histologic Grade\"] = self.csv[\"Neoplasm Histologic Grade\"].replace({\"G2\": \"low grade glioma\", \"G3\": \"high grade glioma\"})\n",
    "        \n",
    "        # Replace IDH status values\n",
    "        self.csv[\"Subtype\"] = self.csv[\"Subtype\"].replace({\n",
    "            \"LGG_IDHmut-non-codel\": \"IDH mutation\",\n",
    "            \"LGG_IDHmut-codel\": \"IDH mutation\",\n",
    "            \"LGG_IDHwt\": \"wild-type IDH\"\n",
    "        })\n",
    "\n",
    "        \n",
    "        self.folder = folder\n",
    "        self.magnification = magnification\n",
    "        self.transform = transform\n",
    "        self.n_patches = n_patches\n",
    "        self.random_selection = random_selection\n",
    "        self.slide_ids = self.csv[\"uuid\"].unique()\n",
    "        success_ids = load_success_ids(self.folder)\n",
    "        self.slide_ids = [x for x in self.slide_ids if x in success_ids]\n",
    "        if limit:\n",
    "            self.slide_ids = self.slide_ids[:limit]\n",
    "        self.labels = []\n",
    "        self.patches = []\n",
    "        self.load_patches()\n",
    "        self.compute_weights()\n",
    "        \n",
    "    def load_patches(self):\n",
    "        \"\"\"\n",
    "        Load n_patches into memory.\n",
    "        \"\"\"\n",
    "        for slide_id in tqdm(self.slide_ids, desc=\"Prefetch patches\"):\n",
    "            # TODO: adjust `_features.h5` once we renamed it on the storage server\n",
    "            file = f\"{self.folder}/{slide_id}_features.h5\"\n",
    "            try:\n",
    "                with h5py.File(file, \"r\") as h5f:\n",
    "                    n_patches = min(self.n_patches, len(h5f[str(self.magnification)]))\n",
    "                    # select random indices\n",
    "                    if self.random_selection:\n",
    "                        indices = sample(range(n_patches), n_patches)\n",
    "                    else:\n",
    "                        indices = list(range(n_patches))\n",
    "                    imgs = [\n",
    "                        Image.fromarray(h5f[str(self.magnification)][i]) for i in indices\n",
    "                    ]\n",
    "                    self.patches.append((imgs, slide_id))\n",
    "                    self.labels.append(self.get_label(slide_id))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    def get_label(self, slide_id):\n",
    "        return self.csv.loc[self.csv[\"uuid\"] == slide_id, \"label\"].values[0]\n",
    "    def get_metadata(self, slide_id):\n",
    "        return self.csv.loc[self.csv[\"uuid\"] == slide_id]\n",
    "    def compute_weights(self):\n",
    "        \"\"\"\n",
    "        Compute weights for WeightedRandomSampler.\n",
    "        \"\"\"\n",
    "        class_counts = {}\n",
    "        for label in self.labels:\n",
    "            if label in class_counts:\n",
    "                class_counts[label] += 1\n",
    "            else:\n",
    "                class_counts[label] = 1\n",
    "        class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "        self.weights = [class_weights[label] for label in self.labels]\n",
    "    def __getitem__(self, idx):\n",
    "        imgs, slide_id = self.patches[idx]\n",
    "        imgs = [self.transform(img) for img in imgs]\n",
    "        label = self.get_label(slide_id)\n",
    "        metadata = self.get_metadata(slide_id)\n",
    "        age = metadata[\"Diagnosis Age\"].values[0]\n",
    "        race = metadata[\"Race Category\"].values[0]\n",
    "        sex = metadata[\"Sex\"].values[0]\n",
    "        grade = metadata[\"Neoplasm Histologic Grade\"].values[0]\n",
    "        IDHstatus = metadata[\"Subtype\"].values[0]\n",
    "        tumortype = metadata[\"Tumor Type\"].values[0]\n",
    "        prompt = f\"a frozen brain histopathology slide of a {race.lower()}, {sex.lower()}, age {age}, has {IDHstatus.lower()}, {grade.lower()} of {tumortype.lower()}\"\n",
    "        return slide_id, imgs, label, prompt\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: Loading success IDs from deprecated success.txt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Prefetch patches: 100%|███████████████████████| 536/536 [01:32<00:00,  5.82it/s]\n"
     ]
    }
   ],
   "source": [
    "folder = \"/n/data2/hms/dbmi/kyu/lab/che099/data/tcga_lgg/frozen_patches_20x\"\n",
    "csv = \"/n/data2/hms/dbmi/kyu/lab/che099/data/idil_tcga_lgg_merge_idh.csv\"\n",
    "\n",
    "assert os.path.exists(folder)\n",
    "assert os.path.exists(csv)\n",
    "dataset = IdilDataSet(csv, folder=folder, magnification=20, random_selection=True, limit=None, wsi_type=\"frozen\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "check_min_version(\"0.12.0.dev0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = get_logger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def save_model_card(\n",
    "    repo_name, images=None, base_model=str, prompt=str, repo_folder=None\n",
    "):\n",
    "    img_str = \"\"\n",
    "    for i, image in enumerate(images):\n",
    "        image.save(os.path.join(repo_folder, f\"image_{i}.png\"))\n",
    "        img_str += f\"![img_{i}](./image_{i}.png)\\n\"\n",
    "    yaml = f\"\"\"\n",
    "---\n",
    "license: creativeml-openrail-m\n",
    "base_model: {base_model}\n",
    "tags:\n",
    "- stable-diffusion\n",
    "- stable-diffusion-diffusers\n",
    "- text-to-image\n",
    "- diffusers\n",
    "- lora\n",
    "inference: true\n",
    "---\n",
    "    \"\"\"\n",
    "    model_card = f\"\"\"\n",
    "# LoRA DreamBooth - {repo_name}\n",
    "These are LoRA adaption weights for {repo_name}. The weights were trained on {prompt} using [DreamBooth](https://dreambooth.github.io/). You can find some example images in the following. \\n\n",
    "{img_str}\n",
    "\"\"\"\n",
    "    with open(os.path.join(repo_folder, \"README.md\"), \"w\") as f:\n",
    "        f.write(yaml + model_card)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def import_model_class_from_model_name_or_path(\n",
    "    pretrained_model_name_or_path: str, revision: str\n",
    "):\n",
    "    text_encoder_config = PretrainedConfig.from_pretrained(\n",
    "        pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=revision,\n",
    "    )\n",
    "    model_class = text_encoder_config.architectures[0]\n",
    "    if model_class == \"CLIPTextModel\":\n",
    "        from transformers import CLIPTextModel\n",
    "        return CLIPTextModel\n",
    "    elif model_class == \"RobertaSeriesModelWithTransformation\":\n",
    "        from diffusers.pipelines.alt_diffusion.modeling_roberta_series import (\n",
    "            RobertaSeriesModelWithTransformation,\n",
    "        )\n",
    "        return RobertaSeriesModelWithTransformation\n",
    "    else:\n",
    "        raise ValueError(f\"{model_class} is not supported.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_args(input_args=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--revision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"A folder containing the training data of instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"A folder containing the training data of class images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"The prompt with identifier specifying the instance\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The prompt to specify images in the same class as provided instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"A prompt that is used during validation to verify that the model is learning.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_validation_images\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of images that should be generated during validation with `validation_prompt`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_epochs\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=(\n",
    "            \"Run dreambooth validation every X epochs. Dreambooth validation consists of running the prompt\"\n",
    "            \" `args.validation_prompt` multiple times: `args.num_validation_images`.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--with_prior_preservation\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Flag to add prior preservation loss.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prior_loss_weight\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"The weight of prior preservation loss.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_class_images\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=(\n",
    "            \"Minimal class images for prior preservation loss. If there are not enough images already present in\"\n",
    "            \" class_data_dir, additional images will be sampled with class_prompt.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"lora-dreambooth-model\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=None, help=\"A seed for reproducible training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n",
    "            \" resolution\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--center_crop\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n",
    "            \" cropped. The images will be resized to the resolution first before cropping.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sample_batch_size\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Batch size (per device) for sampling images.\",\n",
    "    )\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates. These checkpoints can be used both as final\"\n",
    "            \" checkpoints in case they are better than the last checkpoint, and are also suitable for resuming\"\n",
    "            \" training using `--resume_from_checkpoint`.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=(\n",
    "            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n",
    "            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_checkpointing\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-4,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--scale_lr\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler\",\n",
    "        type=str,\n",
    "        default=\"constant\",\n",
    "        help=(\n",
    "            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n",
    "            ' \"constant\", \"constant_with_warmup\"]'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_warmup_steps\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Number of steps for the warmup in the lr scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_num_cycles\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_power\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Power factor of the polynomial scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_8bit_adam\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use 8-bit Adam from bitsandbytes.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_beta1\",\n",
    "        type=float,\n",
    "        default=0.9,\n",
    "        help=\"The beta1 parameter for the Adam optimizer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_beta2\",\n",
    "        type=float,\n",
    "        default=0.999,\n",
    "        help=\"The beta2 parameter for the Adam optimizer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_epsilon\",\n",
    "        type=float,\n",
    "        default=1e-08,\n",
    "        help=\"Epsilon value for the Adam optimizer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--push_to_hub\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to push the model to the Hub.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hub_token\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The token to use to push to the Model Hub.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hub_model_id\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--allow_tf32\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n",
    "            \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prior_generation_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp32\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--local_rank\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help=\"For distributed training: local_rank\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--enable_xformers_memory_efficient_attention\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use xformers.\",\n",
    "    )\n",
    "    if input_args is not None:\n",
    "        args = parser.parse_args(input_args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "    if args.with_prior_preservation:\n",
    "        if args.class_data_dir is None:\n",
    "            raise ValueError(\"You must specify a data directory for class images.\")\n",
    "        if args.class_prompt is None:\n",
    "            raise ValueError(\"You must specify prompt for class images.\")\n",
    "    else:\n",
    "        # logger is not available yet\n",
    "        if args.class_data_dir is not None:\n",
    "            warnings.warn(\n",
    "                \"You need not use --class_data_dir without --with_prior_preservation.\"\n",
    "            )\n",
    "        if args.class_prompt is not None:\n",
    "            warnings.warn(\n",
    "                \"You need not use --class_prompt without --with_prior_preservation.\"\n",
    "            )\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "class DreamBoothDataset(Dataset):\n",
    "#     \"\"\"\n",
    "#     A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "#     It pre-processes the images and the tokenizes prompts.\n",
    "#     \"\"\"\n",
    "#     def __init__(\n",
    "#         self,\n",
    "#         instance_data_root,\n",
    "#         instance_prompt,\n",
    "#         tokenizer,\n",
    "#         class_data_root=None,\n",
    "#         class_prompt=None,\n",
    "#         size=512,\n",
    "#         center_crop=False,\n",
    "#     ):\n",
    "#         self.size = size\n",
    "#         self.center_crop = center_crop\n",
    "#         self.tokenizer = tokenizer\n",
    "\n",
    "#         # self.instance_data_root = Path(instance_data_root)\n",
    "#         # if not self.instance_data_root.exists():\n",
    "#         # raise ValueError(\"Instance images root doesn't exists.\")\n",
    "#         self.instance_images_path = glob(instance_data_root)\n",
    "#         print(\"line427, number of image: {}\".format(len(self.instance_images_path)))\n",
    "\n",
    "#         # self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
    "#         self.num_instance_images = len(self.instance_images_path)\n",
    "#         self.instance_prompt = instance_prompt\n",
    "#         self._length = self.num_instance_images\n",
    "#         if class_data_root is not None:\n",
    "#             self.class_data_root = Path(class_data_root)\n",
    "#             self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "#             self.class_images_path = list(self.class_data_root.iterdir())\n",
    "#             self.num_class_images = len(self.class_images_path)\n",
    "#             self._length = max(self.num_class_images, self.num_instance_images)\n",
    "#             self.class_prompt = class_prompt\n",
    "#         else:\n",
    "#             self.class_data_root = None\n",
    "#         self.image_transforms = transforms.Compose(\n",
    "#             [\n",
    "#                 transforms.Resize(\n",
    "#                     size, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "#                 ),\n",
    "#                 (\n",
    "#                     transforms.CenterCrop(size)\n",
    "#                     if center_crop\n",
    "#                     else transforms.RandomCrop(size)\n",
    "#                 ),\n",
    "#                 transforms.ToTensor(),\n",
    "#                 transforms.Normalize([0.5], [0.5]),\n",
    "#             ]\n",
    "#         )\n",
    "#     def __len__(self):\n",
    "#         return self._length\n",
    "#     def __getitem__(self, index):\n",
    "#         example = {}\n",
    "#         instance_image = Image.open(\n",
    "#             self.instance_images_path[index % self.num_instance_images]\n",
    "#         )\n",
    "#         if not instance_image.mode == \"RGB\":\n",
    "#             instance_image = instance_image.convert(\"RGB\")\n",
    "#         example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "#         example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "#             self.instance_prompt,\n",
    "#             truncation=True,\n",
    "#             padding=\"max_length\",\n",
    "#             max_length=self.tokenizer.model_max_length,\n",
    "#             return_tensors=\"pt\",\n",
    "#         ).input_ids\n",
    "#         if self.class_data_root:\n",
    "#             class_image = Image.open(\n",
    "#                 self.class_images_path[index % self.num_class_images]\n",
    "#             )\n",
    "#             if not class_image.mode == \"RGB\":\n",
    "#                 class_image = class_image.convert(\"RGB\")\n",
    "#             example[\"class_images\"] = self.image_transforms(class_image)\n",
    "#             example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "#                 self.class_prompt,\n",
    "#                 truncation=True,\n",
    "#                 padding=\"max_length\",\n",
    "#                 max_length=self.tokenizer.model_max_length,\n",
    "#                 return_tensors=\"pt\",\n",
    "#             ).input_ids\n",
    "#         return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def collate_fn(examples, with_prior_preservation=False):\n",
    "    input_ids = [example[3] for example in examples]  # Assuming the prompt is the fourth element\n",
    "    \n",
    "    # Extract images from the examples\n",
    "    pixel_values = [example[1] for example in examples]  # Assuming the images are the second element\n",
    "\n",
    "    if with_prior_preservation:\n",
    "        # Handle prior preservation examples if needed \n",
    "        input_ids += [example[\"class_prompt\"] for example in examples]\n",
    "        pixel_values += [example[\"class_images\"] for example in examples]\n",
    "\n",
    "    # Stack pixel values into a batch tensor\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    # Tokenize input prompts into input_ids\n",
    "    input_ids = tokenizer(input_ids, padding=True, return_tensors=\"pt\").input_ids\n",
    "\n",
    "    # Create a batch dictionary\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": pixel_values,\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_repo_name(\n",
    "    model_id: str, organization: Optional[str] = None, token: Optional[str] = None\n",
    "):\n",
    "    if token is None:\n",
    "        token = HfFolder.get_token()\n",
    "    if organization is None:\n",
    "        username = whoami(token)[\"name\"]\n",
    "        return f\"{username}/{model_id}\"\n",
    "    else:\n",
    "        return f\"{organization}/{model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import wandb\n",
    "\n",
    "def main(args):\n",
    "    logging_dir = Path(args.output_dir, \"logs\")  # Corrected logging directory initialization\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        logging_dir=logging_dir,\n",
    "    )\n",
    "\n",
    "    if args.report_to == \"wandb\":\n",
    "        if not wandb_available():\n",
    "            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n",
    "        wandb.init(project=\"your_project_name\", config=args)\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state, main_process_only=False)\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "        diffusers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "        diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed\n",
    "\n",
    "    train_dataset = IdilDataSet(\n",
    "        csv_path=args.instance_data_dir,\n",
    "        folder=args.class_data_dir if args.with_prior_preservation else args.instance_data_dir,\n",
    "        magnification=20,\n",
    "        transform=get_transforms(train=True),\n",
    "        n_patches=250,\n",
    "        random_selection=True,\n",
    "        limit=None,\n",
    "        wsi_type=\"frozen\"\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "    \n",
    "    # Generate class images if prior preservation is enabled.\n",
    "    if args.with_prior_preservation:\n",
    "        class_images_dir = Path(args.class_data_dir)\n",
    "        if not class_images_dir.exists():\n",
    "            class_images_dir.mkdir(parents=True)\n",
    "        cur_class_images = len(list(class_images_dir.iterdir()))\n",
    "        if cur_class_images < args.num_class_images:\n",
    "            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n",
    "            if args.prior_generation_precision == \"fp32\":\n",
    "                torch_dtype = torch.float32\n",
    "            elif args.prior_generation_precision == \"fp16\":\n",
    "                torch_dtype = torch.float16\n",
    "            elif args.prior_generation_precision == \"bf16\":\n",
    "                torch_dtype = torch.bfloat16\n",
    "            pipeline = DiffusionPipeline.from_pretrained(\n",
    "                args.pretrained_model_name_or_path,\n",
    "                torch_dtype=torch_dtype,\n",
    "                safety_checker=None,\n",
    "                revision=args.revision,\n",
    "            )\n",
    "            pipeline.set_progress_bar_config(disable=True)\n",
    "            num_new_images = args.num_class_images - cur_class_images\n",
    "            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n",
    "            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n",
    "            sample_dataloader = torch.utils.data.DataLoader(\n",
    "                sample_dataset, batch_size=args.sample_batch_size\n",
    "            )\n",
    "            sample_dataloader = accelerator.prepare(sample_dataloader)\n",
    "            pipeline.to(accelerator.device)\n",
    "            for example in tqdm(\n",
    "                sample_dataloader,\n",
    "                desc=\"Generating class images\",\n",
    "                disable=not accelerator.is_local_main_process,\n",
    "            ):\n",
    "                images = pipeline(example[\"prompt\"]).images\n",
    "                for i, image in enumerate(images):\n",
    "                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n",
    "                    image_filename = (\n",
    "                        class_images_dir\n",
    "                        / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n",
    "                    )\n",
    "                    image.save(image_filename)\n",
    "            del pipeline\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if args.push_to_hub:\n",
    "            if args.hub_model_id is None:\n",
    "                repo_name = get_full_repo_name(\n",
    "                    Path(args.output_dir).name, token=args.hub_token\n",
    "                )\n",
    "            else:\n",
    "                repo_name = args.hub_model_id\n",
    "            create_repo(repo_name, exist_ok=True, token=args.hub_token)\n",
    "            repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n",
    "            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "                if \"step_*\" not in gitignore:\n",
    "                    gitignore.write(\"step_*\\n\")\n",
    "                if \"epoch_*\" not in gitignore:\n",
    "                    gitignore.write(\"epoch_*\\n\")\n",
    "        elif args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    # Load the tokenizer\n",
    "    if args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\n",
    "    elif args.pretrained_model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args.pretrained_model_name_or_path,\n",
    "            subfolder=\"tokenizer\",\n",
    "            revision=args.revision,\n",
    "            use_fast=False,\n",
    "        )\n",
    "\n",
    "    # import correct text encoder class\n",
    "    text_encoder_cls = import_model_class_from_model_name_or_path(\n",
    "        args.pretrained_model_name_or_path, args.revision\n",
    "    )\n",
    "\n",
    "    # Load scheduler and models\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"scheduler\"\n",
    "    )\n",
    "    text_encoder = text_encoder_cls.from_pretrained(\n",
    "        args.pretrained_model_name_or_path,\n",
    "        subfolder=\"text_encoder\",\n",
    "        revision=args.revision,\n",
    "    )\n",
    "    vae = AutoencoderKL.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision\n",
    "    )\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n",
    "    )\n",
    "\n",
    "    # We only train the additional adapter LoRA layers\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "\n",
    "    # For mixed precision training we cast the text_encoder and vae weights to half-precision\n",
    "    # as these models are only used for inference, keeping weights in full precision is not required.\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    # Move unet, vae and text_encoder to device and cast to weight_dtype\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        if is_xformers_available():\n",
    "            unet.enable_xformers_memory_efficient_attention()\n",
    "        else:\n",
    "            raise ValueError(\n",
    "                \"xformers is not available. Make sure it is installed correctly\"\n",
    "            )\n",
    "\n",
    "    lora_attn_procs = {}\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = (\n",
    "            None\n",
    "            if name.endswith(\"attn1.processor\")\n",
    "            else unet.config.cross_attention_dim\n",
    "        )\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "        lora_attn_procs[name] = LoRACrossAttnProcessor(\n",
    "            hidden_size=hidden_size, cross_attention_dim=cross_attention_dim\n",
    "        )\n",
    "    unet.set_attn_processor(lora_attn_procs)\n",
    "    lora_layers = AttnProcsLayers(unet.attn_processors)\n",
    "    accelerator.register_for_checkpointing(lora_layers)\n",
    "    if args.scale_lr:\n",
    "        args.learning_rate = (\n",
    "            args.learning_rate\n",
    "            * args.gradient_accumulation_steps\n",
    "            * args.train_batch_size\n",
    "            * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    # Enable TF32 for faster training on Ampere GPUs\n",
    "    if args.allow_tf32:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    # Use 8-bit Adam for lower memory usage or to fine-tune the model in 16GB GPUs\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
    "            )\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    # Optimizer creation\n",
    "    optimizer = optimizer_class(\n",
    "        lora_layers.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "    )\n",
    "\n",
    "    # Scheduler and math around the number of training steps.\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "    lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    "        num_cycles=args.lr_num_cycles,\n",
    "        power=args.lr_power,\n",
    "    )\n",
    "\n",
    "    # Prepare everything with our `accelerator`.\n",
    "    lora_layers, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        lora_layers, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    # We need to recalculate our total training steps as the size of the training dataloader may have changed.\n",
    "    num_update_steps_per_epoch = math.ceil(\n",
    "        len(train_dataloader) / args.gradient_accumulation_steps\n",
    "    )\n",
    "    if overrode_max_train_steps:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    # Afterwards we recalculate our number of training epochs\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    # We need to initialize the trackers we use, and also store our configuration.\n",
    "    # The trackers initializes automatically on the main process.\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"dreambooth-lora\", config=vars(args))\n",
    "\n",
    "    # Train!\n",
    "    total_batch_size = (\n",
    "        args.train_batch_size\n",
    "        * accelerator.num_processes\n",
    "        * args.gradient_accumulation_steps\n",
    "    )\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "    logger.info(\n",
    "        f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\"\n",
    "    )\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "\n",
    "    # Potentially load in the weights and states from a previous save\n",
    "    if args.resume_from_checkpoint:\n",
    "        if args.resume_from_checkpoint != \"latest\":\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            # Get the mos recent checkpoint\n",
    "            dirs = os.listdir(args.output_dir)\n",
    "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            path = dirs[-1] if len(dirs) > 0 else None\n",
    "        if path is None:\n",
    "            accelerator.print(\n",
    "                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "            )\n",
    "            args.resume_from_checkpoint = None\n",
    "        else:\n",
    "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "            accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "            global_step = int(path.split(\"-\")[1])\n",
    "            resume_global_step = global_step * args.gradient_accumulation_steps\n",
    "            first_epoch = global_step // num_update_steps_per_epoch\n",
    "            resume_step = resume_global_step % (\n",
    "                num_update_steps_per_epoch * args.gradient_accumulation_steps\n",
    "            )\n",
    "\n",
    "    # Only show the progress bar once on each machine.\n",
    "    progress_bar = tqdm(\n",
    "        range(global_step, args.max_train_steps),\n",
    "        disable=not accelerator.is_local_main_process,\n",
    "    )\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "    for epoch in range(first_epoch, args.num_train_epochs):\n",
    "        unet.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            # Skip steps until we reach the resumed step\n",
    "            if (\n",
    "                args.resume_from_checkpoint\n",
    "                and epoch == first_epoch\n",
    "                and step < resume_step\n",
    "            ):\n",
    "                if step % args.gradient_accumulation_steps == 0:\n",
    "                    progress_bar.update(1)\n",
    "                continue\n",
    "            with accelerator.accumulate(unet):\n",
    "                # Convert images to latent space\n",
    "                latents = vae.encode(\n",
    "                    batch[\"pixel_values\"].to(dtype=weight_dtype)\n",
    "                ).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "                # Sample noise that we'll add to the latents\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                # Sample a random timestep for each image\n",
    "                timesteps = torch.randint(\n",
    "                    0,\n",
    "                    noise_scheduler.config.num_train_timesteps,\n",
    "                    (bsz,),\n",
    "                    device=latents.device,\n",
    "                )\n",
    "                timesteps = timesteps.long()\n",
    "                # Add noise to the latents according to the noise magnitude at each timestep\n",
    "                # (this is the forward diffusion process)\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "                # Get the text embedding for conditioning\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "                # Predict the noise residual\n",
    "                model_pred = unet(\n",
    "                    noisy_latents, timesteps, encoder_hidden_states\n",
    "                ).sample\n",
    "                # Get the target for loss depending on the prediction type\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(\n",
    "                        f\"Unknown prediction type {noise_scheduler.config.prediction_type}\"\n",
    "                    )\n",
    "                if args.with_prior_preservation:\n",
    "                    # Chunk the noise and model_pred into two parts and compute the loss on each part separately.\n",
    "                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "                    target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "                    # Compute instance loss\n",
    "                    loss = F.mse_loss(\n",
    "                        model_pred.float(), target.float(), reduction=\"mean\"\n",
    "                    )\n",
    "                    # Compute prior loss\n",
    "                    prior_loss = F.mse_loss(\n",
    "                        model_pred_prior.float(), target_prior.float(), reduction=\"mean\"\n",
    "                    )\n",
    "                    # Add the prior loss to the instance loss.\n",
    "                    loss = loss + args.prior_loss_weight * prior_loss\n",
    "                else:\n",
    "                    loss = F.mse_loss(\n",
    "                        model_pred.float(), target.float(), reduction=\"mean\"\n",
    "                    )\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    params_to_clip = lora_layers.parameters()\n",
    "                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            # Checks if the accelerator has performed an optimization step behind the scenes\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    if accelerator.is_main_process:\n",
    "                        save_path = os.path.join(\n",
    "                            args.output_dir, f\"checkpoint-{global_step}\"\n",
    "                        )\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "        if args.validation_prompt is not None and epoch % args.validation_epochs == 0:\n",
    "            logger.info(\n",
    "                f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n",
    "                f\" {args.validation_prompt}.\"\n",
    "            )\n",
    "            # create pipeline\n",
    "            pipeline = DiffusionPipeline.from_pretrained(\n",
    "                args.pretrained_model_name_or_path,\n",
    "                unet=accelerator.unwrap_model(unet),\n",
    "                text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "                revision=args.revision,\n",
    "                torch_dtype=weight_dtype,\n",
    "            )\n",
    "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "                pipeline.scheduler.config\n",
    "            )\n",
    "            pipeline = pipeline.to(accelerator.device)\n",
    "            pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "            # run inference\n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(\n",
    "                args.seed\n",
    "            )\n",
    "            prompt = args.num_validation_images * [args.validation_prompt]\n",
    "            images = pipeline(\n",
    "                prompt, num_inference_steps=25, generator=generator\n",
    "            ).images\n",
    "            for tracker in accelerator.trackers:\n",
    "                if tracker.name == \"tensorboard\":\n",
    "                    np_images = np.stack([np.asarray(img) for img in images])\n",
    "                    tracker.writer.add_images(\n",
    "                        \"validation\", np_images, epoch, dataformats=\"NHWC\"\n",
    "                    )\n",
    "                if tracker.name == \"wandb\":\n",
    "                    tracker.log(\n",
    "                        {\n",
    "                            \"validation\": [\n",
    "                                wandb.Image(\n",
    "                                    image, caption=f\"{i}: {args.validation_prompt}\"\n",
    "                                )\n",
    "                                for i, image in enumerate(images)\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "            del pipeline\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    # Save the lora layers\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unet = unet.to(torch.float32)\n",
    "        unet.save_attn_procs(args.output_dir)\n",
    "\n",
    "        # Final inference\n",
    "        # Load previous pipeline\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            args.pretrained_model_name_or_path,\n",
    "            revision=args.revision,\n",
    "            torch_dtype=weight_dtype,\n",
    "        )\n",
    "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(\n",
    "            pipeline.scheduler.config\n",
    "        )\n",
    "        pipeline = pipeline.to(accelerator.device)\n",
    "\n",
    "        # load attention processors\n",
    "        pipeline.unet.load_attn_procs(args.output_dir)\n",
    "\n",
    "        # run inference\n",
    "        if args.validation_prompt and args.num_validation_images > 0:\n",
    "            generator = (\n",
    "                torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "                if args.seed\n",
    "                else None\n",
    "            )\n",
    "            prompt = args.num_validation_images * [args.validation_prompt]\n",
    "            images = pipeline(\n",
    "                prompt, num_inference_steps=25, generator=generator\n",
    "            ).images\n",
    "            test_image_dir = Path(args.output_dir) / \"test_images\"\n",
    "            test_image_dir.mkdir()\n",
    "            for i, image in enumerate(images):\n",
    "                out_path = test_image_dir / f\"image_{i}.png\"\n",
    "                image.save(out_path)\n",
    "            for tracker in accelerator.trackers:\n",
    "                if tracker.name == \"tensorboard\":\n",
    "                    np_images = np.stack([np.asarray(img) for img in images])\n",
    "                    tracker.writer.add_images(\n",
    "                        \"test\", np_images, epoch, dataformats=\"NHWC\"\n",
    "                    )\n",
    "                if tracker.name == \"wandb\":\n",
    "                    tracker.log(\n",
    "                        {\n",
    "                            \"test\": [\n",
    "                                wandb.Image(\n",
    "                                    image, caption=f\"{i}: {args.validation_prompt}\"\n",
    "                                )\n",
    "                                for i, image in enumerate(images)\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "        if args.push_to_hub:\n",
    "            save_model_card(\n",
    "                repo_name,\n",
    "                images=images,\n",
    "                base_model=args.pretrained_model_name_or_path,\n",
    "                prompt=args.instance_prompt,\n",
    "                repo_folder=args.output_dir,\n",
    "            )\n",
    "            repo.push_to_hub(\n",
    "                commit_message=\"End of training\", blocking=False, auto_lfs_prune=True\n",
    "            )\n",
    "    accelerator.end_training()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --pretrained_model_name_or_path\n",
      "                             PRETRAINED_MODEL_NAME_OR_PATH\n",
      "                             [--revision REVISION]\n",
      "                             [--tokenizer_name TOKENIZER_NAME]\n",
      "                             --instance_data_dir INSTANCE_DATA_DIR\n",
      "                             [--class_data_dir CLASS_DATA_DIR]\n",
      "                             --instance_prompt INSTANCE_PROMPT\n",
      "                             [--class_prompt CLASS_PROMPT]\n",
      "                             [--validation_prompt VALIDATION_PROMPT]\n",
      "                             [--num_validation_images NUM_VALIDATION_IMAGES]\n",
      "                             [--validation_epochs VALIDATION_EPOCHS]\n",
      "                             [--with_prior_preservation]\n",
      "                             [--prior_loss_weight PRIOR_LOSS_WEIGHT]\n",
      "                             [--num_class_images NUM_CLASS_IMAGES]\n",
      "                             [--output_dir OUTPUT_DIR] [--seed SEED]\n",
      "                             [--resolution RESOLUTION] [--center_crop]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--sample_batch_size SAMPLE_BATCH_SIZE]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_train_steps MAX_TRAIN_STEPS]\n",
      "                             [--checkpointing_steps CHECKPOINTING_STEPS]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--gradient_checkpointing]\n",
      "                             [--learning_rate LEARNING_RATE] [--scale_lr]\n",
      "                             [--lr_scheduler LR_SCHEDULER]\n",
      "                             [--lr_warmup_steps LR_WARMUP_STEPS]\n",
      "                             [--lr_num_cycles LR_NUM_CYCLES]\n",
      "                             [--lr_power LR_POWER]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--use_8bit_adam] [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_weight_decay ADAM_WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM] [--push_to_hub]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--logging_dir LOGGING_DIR] [--allow_tf32]\n",
      "                             [--report_to REPORT_TO]\n",
      "                             [--mixed_precision {no,fp16,bf16}]\n",
      "                             [--prior_generation_precision {no,fp32,fp16,bf16}]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--enable_xformers_memory_efficient_attention]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --pretrained_model_name_or_path, --instance_data_dir, --instance_prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idu675/.conda/envs/notebookenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
