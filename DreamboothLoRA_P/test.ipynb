{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import hashlib\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "import datasets\n",
    "import diffusers\n",
    "import transformers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import (\n",
    "    AutoencoderKL,\n",
    "    DDPMScheduler,\n",
    "    DiffusionPipeline,\n",
    "    DPMSolverMultistepScheduler,\n",
    "    UNet2DConditionModel,\n",
    ")\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.cross_attention import LoRACrossAttnProcessor\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from diffusers.utils.import_utils import is_xformers_available\n",
    "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a95a17bb618549e8afdda0f3209fb48e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/tokenizer_config.json:   0%|          | 0.00/807 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a59d75c7d3734b0d8c8c375b8f7e9081",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/vocab.json:   0%|          | 0.00/1.06M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "86dff7038c8142e19ee3b841ff074aec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/merges.txt:   0%|          | 0.00/525k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8f5c5fb99f24412694dd5cf3ed9adcbe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer/special_tokens_map.json:   0%|          | 0.00/460 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "line427, number of image: 0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.utils.checkpoint\n",
    "from torch.utils.data import Dataset\n",
    "import datasets\n",
    "import diffusers\n",
    "import transformers\n",
    "from train_dreambooth_lora import DreamBoothDataset\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    \"stabilityai/stable-diffusion-2-1-base\",\n",
    "    subfolder=\"tokenizer\",\n",
    "    # revision=args.revision,\n",
    "    use_fast=False,\n",
    ")\n",
    "\n",
    "instance_data_dir='LU.ruAD_black_formalin_origin/*.jpg'\n",
    "\n",
    "# now we just try to load the dataset here and see what happens\n",
    "train_dataset = DreamBoothDataset(\n",
    "    instance_data_root=instance_data_dir,\n",
    "    instance_prompt=\"LUAD black formalin\",\n",
    "    class_data_root=None,\n",
    "    class_prompt=None,\n",
    "    tokenizer=tokenizer,\n",
    "    size=512,\n",
    "    center_crop=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "def parse_args(input_args=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--revision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"A folder containing the training data of instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"A folder containing the training data of class images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"The prompt with identifier specifying the instance\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The prompt to specify images in the same class as provided instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"A prompt that is used during validation to verify that the model is learning.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_validation_images\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of images that should be generated during validation with `validation_prompt`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_epochs\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=(\n",
    "            \"Run dreambooth validation every X epochs. Dreambooth validation consists of running the prompt\"\n",
    "            \" `args.validation_prompt` multiple times: `args.num_validation_images`.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--with_prior_preservation\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Flag to add prior preservation loss.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prior_loss_weight\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"The weight of prior preservation loss.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_class_images\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=(\n",
    "            \"Minimal class images for prior preservation loss. If there are not enough images already present in\"\n",
    "            \" class_data_dir, additional images will be sampled with class_prompt.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"lora-dreambooth-model\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--seed\", type=int, default=None, help=\"A seed for reproducible training.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=(\n",
    "            \"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n",
    "            \" resolution\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--center_crop\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n",
    "            \" cropped. The images will be resized to the resolution first before cropping.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Batch size (per device) for the training dataloader.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sample_batch_size\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Batch size (per device) for sampling images.\",\n",
    "    )\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=(\n",
    "            \"Save a checkpoint of the training state every X updates. These checkpoints can be used both as final\"\n",
    "            \" checkpoints in case they are better than the last checkpoint, and are also suitable for resuming\"\n",
    "            \" training using `--resume_from_checkpoint`.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=(\n",
    "            \"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n",
    "            ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_checkpointing\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-4,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--scale_lr\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler\",\n",
    "        type=str,\n",
    "        default=\"constant\",\n",
    "        help=(\n",
    "            'The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n",
    "            ' \"constant\", \"constant_with_warmup\"]'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_warmup_steps\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Number of steps for the warmup in the lr scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_num_cycles\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_power\",\n",
    "        type=float,\n",
    "        default=1.0,\n",
    "        help=\"Power factor of the polynomial scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=(\n",
    "            \"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_8bit_adam\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use 8-bit Adam from bitsandbytes.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_beta1\",\n",
    "        type=float,\n",
    "        default=0.9,\n",
    "        help=\"The beta1 parameter for the Adam optimizer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_beta2\",\n",
    "        type=float,\n",
    "        default=0.999,\n",
    "        help=\"The beta2 parameter for the Adam optimizer.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--adam_epsilon\",\n",
    "        type=float,\n",
    "        default=1e-08,\n",
    "        help=\"Epsilon value for the Adam optimizer\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--push_to_hub\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to push the model to the Hub.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hub_token\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The token to use to push to the Model Hub.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--hub_model_id\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The name of the repository to keep in sync with the local `output_dir`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=(\n",
    "            \"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "            \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--allow_tf32\",\n",
    "        action=\"store_true\",\n",
    "        help=(\n",
    "            \"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n",
    "            \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help=(\n",
    "            'The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "            ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "            \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prior_generation_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp32\", \"fp16\", \"bf16\"],\n",
    "        help=(\n",
    "            \"Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "            \" 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.\"\n",
    "        ),\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--local_rank\",\n",
    "        type=int,\n",
    "        default=-1,\n",
    "        help=\"For distributed training: local_rank\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--enable_xformers_memory_efficient_attention\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use xformers.\",\n",
    "    )\n",
    "\n",
    "    if input_args is not None:\n",
    "        args = parser.parse_args(input_args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    if args.with_prior_preservation:\n",
    "        if args.class_data_dir is None:\n",
    "            raise ValueError(\"You must specify a data directory for class images.\")\n",
    "        if args.class_prompt is None:\n",
    "            raise ValueError(\"You must specify prompt for class images.\")\n",
    "    else:\n",
    "        # logger is not available yet\n",
    "        if args.class_data_dir is not None:\n",
    "            warnings.warn(\n",
    "                \"You need not use --class_data_dir without --with_prior_preservation.\"\n",
    "            )\n",
    "        if args.class_prompt is not None:\n",
    "            warnings.warn(\n",
    "                \"You need not use --class_prompt without --with_prior_preservation.\"\n",
    "            )\n",
    "\n",
    "    return args\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(dict_keys(['instance_images', 'instance_prompt_ids']),\n",
       " torch.Size([3, 512, 512]))"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[0].keys(), train_dataset[0][\"instance_images\"].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# so your dataset appears to work, let's try datalaoder next"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from train_dreambooth_lora import collate_fn\n",
    "\n",
    "train_dataloader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=1,\n",
    "    shuffle=True,\n",
    "    collate_fn=lambda examples: collate_fn(examples, False), # <- error comes from here\n",
    "    num_workers=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch = next(iter(train_dataloader)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 3, 512, 512])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "batch[\"instance_images\"].shape # nice [batch_size, 3, resolution, resolution]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DreamBoothDataset(Dataset):\n",
    "    \"\"\"\n",
    "    A dataset to prepare the instance and class images with the prompts for fine-tuning the model.\n",
    "    It pre-processes the images and the tokenizes prompts.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        instance_data_root,\n",
    "        instance_prompt,\n",
    "        tokenizer,\n",
    "        class_data_root=None,\n",
    "        class_prompt=None,\n",
    "        size=512,\n",
    "        center_crop=False,\n",
    "    ):\n",
    "        self.size = size\n",
    "        self.center_crop = center_crop\n",
    "        self.tokenizer = tokenizer\n",
    "\n",
    "        # self.instance_data_root = Path(instance_data_root)\n",
    "        # if not self.instance_data_root.exists():\n",
    "        # raise ValueError(\"Instance images root doesn't exists.\")\n",
    "\n",
    "        self.instance_images_path = glob(instance_data_root)\n",
    "        print(\"line427, number of image: {}\".format(len(self.instance_images_path)))\n",
    "\n",
    "        # self.instance_images_path = list(Path(instance_data_root).iterdir())\n",
    "        self.num_instance_images = len(self.instance_images_path)\n",
    "        self.instance_prompt = instance_prompt\n",
    "        self._length = self.num_instance_images\n",
    "\n",
    "        if class_data_root is not None:\n",
    "            self.class_data_root = Path(class_data_root)\n",
    "            self.class_data_root.mkdir(parents=True, exist_ok=True)\n",
    "            self.class_images_path = list(self.class_data_root.iterdir())\n",
    "            self.num_class_images = len(self.class_images_path)\n",
    "            self._length = max(self.num_class_images, self.num_instance_images)\n",
    "            self.class_prompt = class_prompt\n",
    "        else:\n",
    "            self.class_data_root = None\n",
    "\n",
    "        self.image_transforms = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize(\n",
    "                    size, interpolation=transforms.InterpolationMode.BILINEAR\n",
    "                ),\n",
    "                (\n",
    "                    transforms.CenterCrop(size)\n",
    "                    if center_crop\n",
    "                    else transforms.RandomCrop(size)\n",
    "                ),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.5], [0.5]),\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    def __len__(self):\n",
    "        return self._length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        instance_image = Image.open(\n",
    "            self.instance_images_path[index % self.num_instance_images]\n",
    "        )\n",
    "        if not instance_image.mode == \"RGB\":\n",
    "            instance_image = instance_image.convert(\"RGB\")\n",
    "        example[\"instance_images\"] = self.image_transforms(instance_image)\n",
    "        example[\"instance_prompt_ids\"] = self.tokenizer(\n",
    "            self.instance_prompt,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=self.tokenizer.model_max_length,\n",
    "            return_tensors=\"pt\",\n",
    "        ).input_ids\n",
    "\n",
    "        if self.class_data_root:\n",
    "            class_image = Image.open(\n",
    "                self.class_images_path[index % self.num_class_images]\n",
    "            )\n",
    "            if not class_image.mode == \"RGB\":\n",
    "                class_image = class_image.convert(\"RGB\")\n",
    "            example[\"class_images\"] = self.image_transforms(class_image)\n",
    "            example[\"class_prompt_ids\"] = self.tokenizer(\n",
    "                self.class_prompt,\n",
    "                truncation=True,\n",
    "                padding=\"max_length\",\n",
    "                max_length=self.tokenizer.model_max_length,\n",
    "                return_tensors=\"pt\",\n",
    "            ).input_ids\n",
    "\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_fn(examples, with_prior_preservation=False):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "\n",
    "    # Concat class and instance examples for prior preservation.\n",
    "    # We do this to avoid doing two forward passes.\n",
    "    if with_prior_preservation:\n",
    "        input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
    "        pixel_values += [example[\"class_images\"] for example in examples]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": pixel_values,\n",
    "    }\n",
    "    return batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PromptDataset(Dataset):\n",
    "    \"A simple dataset to prepare the prompts to generate class images on multiple GPUs.\"\n",
    "\n",
    "    def __init__(self, prompt, num_samples):\n",
    "        self.prompt = prompt\n",
    "        self.num_samples = num_samples\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        example = {}\n",
    "        example[\"prompt\"] = self.prompt\n",
    "        example[\"index\"] = index\n",
    "        return example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_repo_name(\n",
    "    model_id: str, organization: Optional[str] = None, token: Optional[str] = None\n",
    "):\n",
    "    if token is None:\n",
    "        token = HfFolder.get_token()\n",
    "    if organization is None:\n",
    "        username = whoami(token)[\"name\"]\n",
    "        return f\"{username}/{model_id}\"\n",
    "    else:\n",
    "        return f\"{organization}/{model_id}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --pretrained_model_name_or_path\n",
      "                             PRETRAINED_MODEL_NAME_OR_PATH\n",
      "ipykernel_launcher.py: error: the following arguments are required: --pretrained_model_name_or_path\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "# argparse.ArgumentParser is the constructor for ArgumentParser class\n",
    "\n",
    "parser.add_argument(\n",
    "    \"--pretrained_model_name_or_path\",\n",
    "    type=str,\n",
    "    default=None,\n",
    "    required=True,\n",
    "    help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    ")\n",
    "\n",
    "args = parser.parse_args(args=[])  # Pass an empty list to avoid parsing command-line arguments\n",
    "args.pretrained_model_name_or_path = \"stabilityai/stable-diffusion-2-1-base\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --pretrained_model_name_or_path\n",
      "                             PRETRAINED_MODEL_NAME_OR_PATH\n",
      "                             [--revision REVISION]\n",
      "                             [--tokenizer_name TOKENIZER_NAME]\n",
      "                             --instance_data_dir INSTANCE_DATA_DIR\n",
      "                             [--class_data_dir CLASS_DATA_DIR]\n",
      "                             --instance_prompt INSTANCE_PROMPT\n",
      "                             [--class_prompt CLASS_PROMPT]\n",
      "                             [--validation_prompt VALIDATION_PROMPT]\n",
      "                             [--num_validation_images NUM_VALIDATION_IMAGES]\n",
      "                             [--validation_epochs VALIDATION_EPOCHS]\n",
      "                             [--with_prior_preservation]\n",
      "                             [--prior_loss_weight PRIOR_LOSS_WEIGHT]\n",
      "                             [--num_class_images NUM_CLASS_IMAGES]\n",
      "                             [--output_dir OUTPUT_DIR] [--seed SEED]\n",
      "                             [--resolution RESOLUTION] [--center_crop]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--sample_batch_size SAMPLE_BATCH_SIZE]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_train_steps MAX_TRAIN_STEPS]\n",
      "                             [--checkpointing_steps CHECKPOINTING_STEPS]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--gradient_checkpointing]\n",
      "                             [--learning_rate LEARNING_RATE] [--scale_lr]\n",
      "                             [--lr_scheduler LR_SCHEDULER]\n",
      "                             [--lr_warmup_steps LR_WARMUP_STEPS]\n",
      "                             [--lr_num_cycles LR_NUM_CYCLES]\n",
      "                             [--lr_power LR_POWER]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--use_8bit_adam] [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_weight_decay ADAM_WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM] [--push_to_hub]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--logging_dir LOGGING_DIR] [--allow_tf32]\n",
      "                             [--report_to REPORT_TO]\n",
      "                             [--mixed_precision {no,fp16,bf16}]\n",
      "                             [--prior_generation_precision {no,fp32,fp16,bf16}]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--enable_xformers_memory_efficient_attention]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --pretrained_model_name_or_path, --instance_data_dir, --instance_prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "args = parse_args()\n",
    "# I need to give it an instance_data_dir as its required\n",
    "# also instance_prompt\n",
    "# and pretrained_model_name_or_path which should indeed come from \n",
    "# def import_model_class_from_model_name_or_path(\n",
    "#     pretrained_model_name_or_path: str, revision: str\n",
    "# ):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'args' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[30], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Scheduler and math around the number of training steps.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m overrode_max_train_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m      3\u001b[0m num_update_steps_per_epoch \u001b[38;5;241m=\u001b[39m math\u001b[38;5;241m.\u001b[39mceil(\n\u001b[0;32m----> 4\u001b[0m     \u001b[38;5;28mlen\u001b[39m(train_dataloader) \u001b[38;5;241m/\u001b[39m \u001b[43margs\u001b[49m\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps\n\u001b[1;32m      5\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m args\u001b[38;5;241m.\u001b[39mmax_train_steps \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m      7\u001b[0m     args\u001b[38;5;241m.\u001b[39mmax_train_steps \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mnum_train_epochs \u001b[38;5;241m*\u001b[39m num_update_steps_per_epoch\n",
      "\u001b[0;31mNameError\u001b[0m: name 'args' is not defined"
     ]
    }
   ],
   "source": [
    "# Scheduler and math around the number of training steps.\n",
    "overrode_max_train_steps = False\n",
    "num_update_steps_per_epoch = math.ceil(\n",
    "    len(train_dataloader) / args.gradient_accumulation_steps\n",
    ")\n",
    "if args.max_train_steps is None:\n",
    "    args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    overrode_max_train_steps = True\n",
    "\n",
    "lr_scheduler = get_scheduler(\n",
    "    args.lr_scheduler,\n",
    "    optimizer=optimizer,\n",
    "    num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "    num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    "    num_cycles=args.lr_num_cycles,\n",
    "    power=args.lr_power,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
