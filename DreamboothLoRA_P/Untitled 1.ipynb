{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e1b5f400-d6f8-4425-9235-f46b4230d823",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:albumentations.check_version:A new version of Albumentations is available: 1.4.10 (you have 1.4.8). Upgrade using: pip install --upgrade albumentations\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding=utf-8\n",
    "#\n",
    "# This file is adapted from https://github.com/huggingface/diffusers/blob/febaf863026bd014b7a14349336544fc109d0f57/examples/dreambooth/train_dreambooth_lora.py\n",
    "\n",
    "import argparse\n",
    "import logging\n",
    "import math\n",
    "import os\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from typing import Optional\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "\n",
    "import diffusers\n",
    "from accelerate import Accelerator\n",
    "from accelerate.logging import get_logger\n",
    "from accelerate.utils import set_seed\n",
    "from diffusers import AutoencoderKL, DDPMScheduler, DiffusionPipeline, DPMSolverMultistepScheduler, UNet2DConditionModel\n",
    "from diffusers.loaders import AttnProcsLayers\n",
    "from diffusers.models.cross_attention import LoRACrossAttnProcessor\n",
    "from diffusers.optimization import get_scheduler\n",
    "from diffusers.utils import check_min_version, is_wandb_available\n",
    "from huggingface_hub import HfFolder, Repository, create_repo, whoami\n",
    "from PIL import Image\n",
    "from torchvision import transforms\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, PretrainedConfig\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "import pandas as pd\n",
    "from typing import List, Optional, Callable\n",
    "import h5py\n",
    "import sqlite3\n",
    "import random\n",
    "\n",
    "# Custom dataset class\n",
    "class IdilDataSet(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: str,\n",
    "        folder: str,\n",
    "        magnification: int,\n",
    "        transform: Optional[Callable] = None,\n",
    "        n_patches: int = 250,\n",
    "        random_selection=False,\n",
    "        limit: Optional[int] = None,\n",
    "        wsi_type: str = \"frozen\"\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.csv = pd.read_csv(csv_path)\n",
    "        self.csv = self.csv[self.csv[\"wsi_type\"] == wsi_type]\n",
    "        self.folder = folder\n",
    "        self.magnification = magnification\n",
    "        self.transform = transform\n",
    "        self.n_patches = n_patches\n",
    "        self.random_selection = random_selection\n",
    "        self.slide_ids = self.csv[\"uuid\"].unique()\n",
    "        success_ids = self.load_success_ids(self.folder)\n",
    "        self.slide_ids = [x for x in self.slide_ids if x in success_ids]\n",
    "        if limit:\n",
    "            self.slide_ids = self.slide_ids[:limit]\n",
    "        self.labels = []\n",
    "        self.patches = []\n",
    "        self.load_patches()\n",
    "        self.compute_weights()\n",
    "    def load_success_ids(self, feat_folder: str):\n",
    "        success_ids = set()\n",
    "        success_txt = f\"{feat_folder}/success.txt\"\n",
    "        success_db = f\"{feat_folder}/success.db\"\n",
    "        if os.path.exists(success_txt):\n",
    "            print(\"Warning: Loading success IDs from deprecated success.txt.\")\n",
    "            with open(success_txt, \"r\") as f:\n",
    "                for line in f:\n",
    "                    success_ids.add(line.strip())\n",
    "        if os.path.exists(success_db):\n",
    "            print(\"Loading success IDs from database.\")\n",
    "            conn = sqlite3.connect(success_db)\n",
    "            cursor = conn.cursor()\n",
    "            cursor.execute(\"SELECT slide_id FROM success\")\n",
    "            success_ids = set([row[0] for row in cursor.fetchall()])\n",
    "            conn.close()\n",
    "        return success_ids\n",
    "    def load_patches(self):\n",
    "        for slide_id in tqdm(self.slide_ids, desc=\"Prefetch patches\"):\n",
    "            file = f\"{self.folder}/{slide_id}_features.h5\"\n",
    "            try:\n",
    "                with h5py.File(file, \"r\") as h5f:\n",
    "                    n_patches = min(self.n_patches, len(h5f[str(self.magnification)]))\n",
    "                    if self.random_selection:\n",
    "                        indices = random.sample(range(n_patches), n_patches)\n",
    "                    else:\n",
    "                        indices = list(range(n_patches))\n",
    "                    imgs = [\n",
    "                        Image.fromarray(h5f[str(self.magnification)][i]) for i in indices\n",
    "                    ]\n",
    "                    self.patches.append((imgs, slide_id))\n",
    "                    self.labels.append(self.get_label(slide_id))\n",
    "            except Exception as e:\n",
    "                pass\n",
    "    def __len__(self):\n",
    "        return len(self.patches)\n",
    "    def get_label(self, slide_id):\n",
    "        return self.csv.loc[self.csv[\"uuid\"] == slide_id, \"label\"].values[0]\n",
    "    def get_metadata(self, slide_id):\n",
    "        return self.csv.loc[self.csv[\"uuid\"] == slide_id]\n",
    "    def compute_weights(self):\n",
    "        class_counts = {}\n",
    "        for label in self.labels:\n",
    "            if label in class_counts:\n",
    "                class_counts[label] += 1\n",
    "            else:\n",
    "                class_counts[label] = 1\n",
    "        class_weights = {cls: 1.0 / count for cls, count in class_counts.items()}\n",
    "        self.weights = [class_weights[label] for label in self.labels]\n",
    "    def __getitem__(self, idx):\n",
    "        imgs, slide_id = self.patches[idx]\n",
    "        if self.transform:\n",
    "            imgs = [self.transform(img) for img in imgs]\n",
    "        label = self.get_label(slide_id)\n",
    "        metadata = self.get_metadata(slide_id)\n",
    "        age = metadata[\"Diagnosis Age\"].values[0]\n",
    "        race = metadata[\"Race Category\"].values[0]\n",
    "        sex = metadata[\"Sex\"].values[0]\n",
    "        return slide_id, imgs, label, age, race, sex\n",
    "\n",
    "# Custom transformations\n",
    "def get_transforms(train=False):\n",
    "    mean = (0.485, 0.456, 0.406)\n",
    "    std = (0.229, 0.224, 0.225)\n",
    "    if train:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.OneOf([\n",
    "                A.Rotate(limit=0, p=1),\n",
    "                A.Rotate(limit=90, p=1),\n",
    "                A.Rotate(limit=180, p=1),\n",
    "                A.Rotate(limit=270, p=1),\n",
    "            ], p=0.5),\n",
    "            A.Compose([\n",
    "                A.OneOf([\n",
    "                    A.ColorJitter(\n",
    "                        brightness=(0.9, 1),\n",
    "                        contrast=(0.9, 1),\n",
    "                        saturation=(0.9, 1),\n",
    "                        hue=(0, 0.1),\n",
    "                        p=1.0,\n",
    "                    ),\n",
    "                    A.Affine(\n",
    "                        scale=(0.5, 1.5),\n",
    "                        translate_percent=(0.0, 0.0),\n",
    "                        shear=(0.5, 1.5),\n",
    "                        p=1.0,\n",
    "                    ),\n",
    "                ], p=0.5),\n",
    "                A.GaussianBlur(\n",
    "                    blur_limit=(1, 3), sigma_limit=(0.1, 3), p=1.0\n",
    "                ),\n",
    "            ]),\n",
    "            A.OneOf([\n",
    "                A.HorizontalFlip(p=0.5),\n",
    "                A.VerticalFlip(p=0.5),\n",
    "            ], p=0.5),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "    else:\n",
    "        return A.Compose([\n",
    "            A.Resize(224, 224),\n",
    "            A.Normalize(mean=mean, std=std),\n",
    "            ToTensorV2(),\n",
    "        ])\n",
    "\n",
    "# Parsing command line arguments\n",
    "def parse_args(input_args=None):\n",
    "    parser = argparse.ArgumentParser(description=\"Simple example of a training script.\")\n",
    "    parser.add_argument(\n",
    "        \"--pretrained_model_name_or_path\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"Path to pretrained model or model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--revision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"Revision of pretrained model identifier from huggingface.co/models.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--tokenizer_name\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Pretrained tokenizer name or path if not the same as model_name\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"A folder containing the training data of instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_data_dir\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=False,\n",
    "        help=\"A folder containing the training data of class images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--instance_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        required=True,\n",
    "        help=\"The prompt with identifier specifying the instance\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--class_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The prompt to specify images in the same class as provided instance images.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_prompt\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"A prompt that is used during validation to verify that the model is learning.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--num_validation_images\",\n",
    "        type=int,\n",
    "        default=4,\n",
    "        help=\"Number of images that should be generated during validation with `validation_prompt`.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--validation_epochs\",\n",
    "        type=int,\n",
    "        default=50,\n",
    "        help=\"Run dreambooth validation every X epochs. Dreambooth validation consists of running the prompt\"\n",
    "             \" `args.validation_prompt` multiple times: `args.num_validation_images`.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--with_prior_preservation\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Flag to add prior preservation loss.\",\n",
    "    )\n",
    "    parser.add_argument(\"--prior_loss_weight\", type=float, default=1.0, help=\"The weight of prior preservation loss.\")\n",
    "    parser.add_argument(\n",
    "        \"--num_class_images\",\n",
    "        type=int,\n",
    "        default=100,\n",
    "        help=\"Minimal class images for prior preservation loss. If there are not enough images already present in\"\n",
    "             \" class_data_dir, additional images will be sampled with class_prompt.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--output_dir\",\n",
    "        type=str,\n",
    "        default=\"lora-dreambooth-model\",\n",
    "        help=\"The output directory where the model predictions and checkpoints will be written.\",\n",
    "    )\n",
    "    parser.add_argument(\"--seed\", type=int, default=None, help=\"A seed for reproducible training.\")\n",
    "    parser.add_argument(\n",
    "        \"--resolution\",\n",
    "        type=int,\n",
    "        default=512,\n",
    "        help=\"The resolution for input images, all the images in the train/validation dataset will be resized to this\"\n",
    "             \" resolution\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--center_crop\",\n",
    "        default=False,\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether to center crop the input images to the resolution. If not set, the images will be randomly\"\n",
    "             \" cropped. The images will be resized to the resolution first before cropping.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--train_batch_size\", type=int, default=4, help=\"Batch size (per device) for the training dataloader.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--sample_batch_size\", type=int, default=4, help=\"Batch size (per device) for sampling images.\"\n",
    "    )\n",
    "    parser.add_argument(\"--num_train_epochs\", type=int, default=1)\n",
    "    parser.add_argument(\n",
    "        \"--max_train_steps\",\n",
    "        type=int,\n",
    "        default=None,\n",
    "        help=\"Total number of training steps to perform.  If provided, overrides num_train_epochs.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--checkpointing_steps\",\n",
    "        type=int,\n",
    "        default=500,\n",
    "        help=\"Save a checkpoint of the training state every X updates. These checkpoints can be used both as final\"\n",
    "             \" checkpoints in case they are better than the last checkpoint, and are also suitable for resuming\"\n",
    "             \" training using `--resume_from_checkpoint`.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--resume_from_checkpoint\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"Whether training should be resumed from a previous checkpoint. Use a path saved by\"\n",
    "             ' `--checkpointing_steps`, or `\"latest\"` to automatically select the last available checkpoint.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_accumulation_steps\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of updates steps to accumulate before performing a backward/update pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--gradient_checkpointing\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to use gradient checkpointing to save memory at the expense of slower backward pass.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--learning_rate\",\n",
    "        type=float,\n",
    "        default=5e-4,\n",
    "        help=\"Initial learning rate (after the potential warmup period) to use.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--scale_lr\",\n",
    "        action=\"store_true\",\n",
    "        default=False,\n",
    "        help=\"Scale the learning rate by the number of GPUs, gradient accumulation steps, and batch size.\",\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_scheduler\",\n",
    "        type=str,\n",
    "        default=\"constant\",\n",
    "        help='The scheduler type to use. Choose between [\"linear\", \"cosine\", \"cosine_with_restarts\", \"polynomial\",'\n",
    "             ' \"constant\", \"constant_with_warmup\"]'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_warmup_steps\", type=int, default=500, help=\"Number of steps for the warmup in the lr scheduler.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--lr_num_cycles\",\n",
    "        type=int,\n",
    "        default=1,\n",
    "        help=\"Number of hard resets of the lr in cosine_with_restarts scheduler.\",\n",
    "    )\n",
    "    parser.add_argument(\"--lr_power\", type=float, default=1.0, help=\"Power factor of the polynomial scheduler.\")\n",
    "    parser.add_argument(\n",
    "        \"--dataloader_num_workers\",\n",
    "        type=int,\n",
    "        default=0,\n",
    "        help=\"Number of subprocesses to use for data loading. 0 means that the data will be loaded in the main process.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--use_8bit_adam\", action=\"store_true\", help=\"Whether or not to use 8-bit Adam from bitsandbytes.\"\n",
    "    )\n",
    "    parser.add_argument(\"--adam_beta1\", type=float, default=0.9, help=\"The beta1 parameter for the Adam optimizer.\")\n",
    "    parser.add_argument(\"--adam_beta2\", type=float, default=0.999, help=\"The beta2 parameter for the Adam optimizer.\")\n",
    "    parser.add_argument(\"--adam_weight_decay\", type=float, default=1e-2, help=\"Weight decay to use.\")\n",
    "    parser.add_argument(\"--adam_epsilon\", type=float, default=1e-08, help=\"Epsilon value for the Adam optimizer\")\n",
    "    parser.add_argument(\"--max_grad_norm\", default=1.0, type=float, help=\"Max gradient norm.\")\n",
    "    parser.add_argument(\"--push_to_hub\", action=\"store_true\", help=\"Whether or not to push the model to the Hub.\")\n",
    "    parser.add_argument(\"--hub_token\", type=str, default=None, help=\"The token to use to push to the Model Hub.\")\n",
    "    parser.add_argument(\n",
    "        \"--hub_model_id\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        help=\"The name of the repository to keep in sync with the local `output_dir`.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--logging_dir\",\n",
    "        type=str,\n",
    "        default=\"logs\",\n",
    "        help=\"[TensorBoard](https://www.tensorflow.org/tensorboard) log directory. Will default to\"\n",
    "             \" *output_dir/runs/**CURRENT_DATETIME_HOSTNAME***.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--allow_tf32\",\n",
    "        action=\"store_true\",\n",
    "        help=\"Whether or not to allow TF32 on Ampere GPUs. Can be used to speed up training. For more information, see\"\n",
    "             \" https://pytorch.org/docs/stable/notes/cuda.html#tensorfloat-32-tf32-on-ampere-devices\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--report_to\",\n",
    "        type=str,\n",
    "        default=\"tensorboard\",\n",
    "        help='The integration to report the results and logs to. Supported platforms are `\"tensorboard\"`'\n",
    "             ' (default), `\"wandb\"` and `\"comet_ml\"`. Use `\"all\"` to report to all integrations.'\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--mixed_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp16\", \"bf16\"],\n",
    "        help=\"Whether to use mixed precision. Choose between fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "             \" 1.10.and an Nvidia Ampere GPU.  Default to the value of accelerate config of the current system or the\"\n",
    "             \" flag passed with the `accelerate.launch` command. Use this argument to override the accelerate config.\"\n",
    "    )\n",
    "    parser.add_argument(\n",
    "        \"--prior_generation_precision\",\n",
    "        type=str,\n",
    "        default=None,\n",
    "        choices=[\"no\", \"fp32\", \"fp16\", \"bf16\"],\n",
    "        help=\"Choose prior generation precision between fp32, fp16 and bf16 (bfloat16). Bf16 requires PyTorch >=\"\n",
    "             \" 1.10.and an Nvidia Ampere GPU.  Default to  fp16 if a GPU is available else fp32.\"\n",
    "    )\n",
    "    parser.add_argument(\"--local_rank\", type=int, default=-1, help=\"For distributed training: local_rank\")\n",
    "    parser.add_argument(\n",
    "        \"--enable_xformers_memory_efficient_attention\", action=\"store_true\", help=\"Whether or not to use xformers.\"\n",
    "    )\n",
    "\n",
    "    if input_args is not None:\n",
    "        args = parser.parse_args(input_args)\n",
    "    else:\n",
    "        args = parser.parse_args()\n",
    "\n",
    "    env_local_rank = int(os.environ.get(\"LOCAL_RANK\", -1))\n",
    "    if env_local_rank != -1 and env_local_rank != args.local_rank:\n",
    "        args.local_rank = env_local_rank\n",
    "\n",
    "    if args.with_prior_preservation:\n",
    "        if args.class_data_dir is None:\n",
    "            raise ValueError(\"You must specify a data directory for class images.\")\n",
    "        if args.class_prompt is None:\n",
    "            raise ValueError(\"You must specify prompt for class images.\")\n",
    "    else:\n",
    "        # logger is not available yet\n",
    "        if args.class_data_dir is not None:\n",
    "            warnings.warn(\"You need not use --class_data_dir without --with_prior_preservation.\")\n",
    "        if args.class_prompt is not None:\n",
    "            warnings.warn(\"You need not use --class_prompt without --with_prior_preservation.\")\n",
    "\n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "83a59992-b275-4417-aa5c-aeede7d328f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] --pretrained_model_name_or_path\n",
      "                             PRETRAINED_MODEL_NAME_OR_PATH\n",
      "                             [--revision REVISION]\n",
      "                             [--tokenizer_name TOKENIZER_NAME]\n",
      "                             --instance_data_dir INSTANCE_DATA_DIR\n",
      "                             [--class_data_dir CLASS_DATA_DIR]\n",
      "                             --instance_prompt INSTANCE_PROMPT\n",
      "                             [--class_prompt CLASS_PROMPT]\n",
      "                             [--validation_prompt VALIDATION_PROMPT]\n",
      "                             [--num_validation_images NUM_VALIDATION_IMAGES]\n",
      "                             [--validation_epochs VALIDATION_EPOCHS]\n",
      "                             [--with_prior_preservation]\n",
      "                             [--prior_loss_weight PRIOR_LOSS_WEIGHT]\n",
      "                             [--num_class_images NUM_CLASS_IMAGES]\n",
      "                             [--output_dir OUTPUT_DIR] [--seed SEED]\n",
      "                             [--resolution RESOLUTION] [--center_crop]\n",
      "                             [--train_batch_size TRAIN_BATCH_SIZE]\n",
      "                             [--sample_batch_size SAMPLE_BATCH_SIZE]\n",
      "                             [--num_train_epochs NUM_TRAIN_EPOCHS]\n",
      "                             [--max_train_steps MAX_TRAIN_STEPS]\n",
      "                             [--checkpointing_steps CHECKPOINTING_STEPS]\n",
      "                             [--resume_from_checkpoint RESUME_FROM_CHECKPOINT]\n",
      "                             [--gradient_accumulation_steps GRADIENT_ACCUMULATION_STEPS]\n",
      "                             [--gradient_checkpointing]\n",
      "                             [--learning_rate LEARNING_RATE] [--scale_lr]\n",
      "                             [--lr_scheduler LR_SCHEDULER]\n",
      "                             [--lr_warmup_steps LR_WARMUP_STEPS]\n",
      "                             [--lr_num_cycles LR_NUM_CYCLES]\n",
      "                             [--lr_power LR_POWER]\n",
      "                             [--dataloader_num_workers DATALOADER_NUM_WORKERS]\n",
      "                             [--use_8bit_adam] [--adam_beta1 ADAM_BETA1]\n",
      "                             [--adam_beta2 ADAM_BETA2]\n",
      "                             [--adam_weight_decay ADAM_WEIGHT_DECAY]\n",
      "                             [--adam_epsilon ADAM_EPSILON]\n",
      "                             [--max_grad_norm MAX_GRAD_NORM] [--push_to_hub]\n",
      "                             [--hub_token HUB_TOKEN]\n",
      "                             [--hub_model_id HUB_MODEL_ID]\n",
      "                             [--logging_dir LOGGING_DIR] [--allow_tf32]\n",
      "                             [--report_to REPORT_TO]\n",
      "                             [--mixed_precision {no,fp16,bf16}]\n",
      "                             [--prior_generation_precision {no,fp32,fp16,bf16}]\n",
      "                             [--local_rank LOCAL_RANK]\n",
      "                             [--enable_xformers_memory_efficient_attention]\n",
      "ipykernel_launcher.py: error: the following arguments are required: --pretrained_model_name_or_path, --instance_data_dir, --instance_prompt\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/idu675/.conda/envs/notebookenv/lib/python3.11/site-packages/IPython/core/interactiveshell.py:3585: UserWarning: To exit: use 'exit', 'quit', or Ctrl-D.\n",
      "  warn(\"To exit: use 'exit', 'quit', or Ctrl-D.\", stacklevel=1)\n"
     ]
    }
   ],
   "source": [
    "def collate_fn(examples, with_prior_preservation=False):\n",
    "    input_ids = [example[\"instance_prompt_ids\"] for example in examples]\n",
    "    pixel_values = [example[\"instance_images\"] for example in examples]\n",
    "\n",
    "    # Concat class and instance examples for prior preservation.\n",
    "    # We do this to avoid doing two forward passes.\n",
    "    if with_prior_preservation:\n",
    "        input_ids += [example[\"class_prompt_ids\"] for example in examples]\n",
    "        pixel_values += [example[\"class_images\"] for example in examples]\n",
    "\n",
    "    pixel_values = torch.stack(pixel_values)\n",
    "    pixel_values = pixel_values.to(memory_format=torch.contiguous_format).float()\n",
    "\n",
    "    input_ids = torch.cat(input_ids, dim=0)\n",
    "\n",
    "    batch = {\n",
    "        \"input_ids\": input_ids,\n",
    "        \"pixel_values\": pixel_values,\n",
    "    }\n",
    "    return batch\n",
    "\n",
    "\n",
    "def main(args):\n",
    "    logging_dir = Path(args.output_dir, args.logging_dir)\n",
    "\n",
    "    accelerator = Accelerator(\n",
    "        gradient_accumulation_steps=args.gradient_accumulation_steps,\n",
    "        mixed_precision=args.mixed_precision,\n",
    "        log_with=args.report_to,\n",
    "        logging_dir=logging_dir,\n",
    "    )\n",
    "\n",
    "    if args.report_to == \"wandb\":\n",
    "        if not is_wandb_available():\n",
    "            raise ImportError(\"Make sure to install wandb if you want to use it for logging during training.\")\n",
    "        import wandb\n",
    "\n",
    "    logging.basicConfig(\n",
    "        format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "        datefmt=\"%m/%d/%Y %H:%M:%S\",\n",
    "        level=logging.INFO,\n",
    "    )\n",
    "    logger.info(accelerator.state, main_process_only=False)\n",
    "    if accelerator.is_local_main_process:\n",
    "        datasets.utils.logging.set_verbosity_warning()\n",
    "        transformers.utils.logging.set_verbosity_warning()\n",
    "        diffusers.utils.logging.set_verbosity_info()\n",
    "    else:\n",
    "        datasets.utils.logging.set_verbosity_error()\n",
    "        transformers.utils.logging.set_verbosity_error()\n",
    "        diffusers.utils.logging.set_verbosity_error()\n",
    "\n",
    "    if args.seed is not None:\n",
    "        set_seed(args.seed)\n",
    "\n",
    "    if args.with_prior_preservation:\n",
    "        class_images_dir = Path(args.class_data_dir)\n",
    "        if not class_images_dir.exists():\n",
    "            class_images_dir.mkdir(parents=True)\n",
    "        cur_class_images = len(list(class_images_dir.iterdir()))\n",
    "\n",
    "        if cur_class_images < args.num_class_images:\n",
    "            torch_dtype = torch.float16 if accelerator.device.type == \"cuda\" else torch.float32\n",
    "            if args.prior_generation_precision == \"fp32\":\n",
    "                torch_dtype = torch.float32\n",
    "            elif args.prior_generation_precision == \"fp16\":\n",
    "                torch_dtype = torch.float16\n",
    "            elif args.prior_generation_precision == \"bf16\":\n",
    "                torch_dtype = torch.bfloat16\n",
    "            pipeline = DiffusionPipeline.from_pretrained(\n",
    "                args.pretrained_model_name_or_path,\n",
    "                torch_dtype=torch_dtype,\n",
    "                safety_checker=None,\n",
    "                revision=args.revision,\n",
    "            )\n",
    "            pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "            num_new_images = args.num_class_images - cur_class_images\n",
    "            logger.info(f\"Number of class images to sample: {num_new_images}.\")\n",
    "\n",
    "            sample_dataset = PromptDataset(args.class_prompt, num_new_images)\n",
    "            sample_dataloader = torch.utils.data.DataLoader(sample_dataset, batch_size=args.sample_batch_size)\n",
    "\n",
    "            sample_dataloader = accelerator.prepare(sample_dataloader)\n",
    "            pipeline.to(accelerator.device)\n",
    "\n",
    "            for example in tqdm(\n",
    "                sample_dataloader, desc=\"Generating class images\", disable=not accelerator.is_local_main_process\n",
    "            ):\n",
    "                images = pipeline(example[\"prompt\"]).images\n",
    "\n",
    "                for i, image in enumerate(images):\n",
    "                    hash_image = hashlib.sha1(image.tobytes()).hexdigest()\n",
    "                    image_filename = class_images_dir / f\"{example['index'][i] + cur_class_images}-{hash_image}.jpg\"\n",
    "                    image.save(image_filename)\n",
    "\n",
    "            del pipeline\n",
    "            if torch.cuda.is_available():\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        if args.push_to_hub:\n",
    "            if args.hub_model_id is None:\n",
    "                repo_name = get_full_repo_name(Path(args.output_dir).name, token=args.hub_token)\n",
    "            else:\n",
    "                repo_name = args.hub_model_id\n",
    "\n",
    "            create_repo(repo_name, exist_ok=True, token=args.hub_token)\n",
    "            repo = Repository(args.output_dir, clone_from=repo_name, token=args.hub_token)\n",
    "\n",
    "            with open(os.path.join(args.output_dir, \".gitignore\"), \"w+\") as gitignore:\n",
    "                if \"step_*\" not in gitignore:\n",
    "                    gitignore.write(\"step_*\\n\")\n",
    "                if \"epoch_*\" not in gitignore:\n",
    "                    gitignore.write(\"epoch_*\\n\")\n",
    "        elif args.output_dir is not None:\n",
    "            os.makedirs(args.output_dir, exist_ok=True)\n",
    "\n",
    "    if args.tokenizer_name:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(args.tokenizer_name, revision=args.revision, use_fast=False)\n",
    "    elif args.pretrained_model_name_or_path:\n",
    "        tokenizer = AutoTokenizer.from_pretrained(\n",
    "            args.pretrained_model_name_or_path,\n",
    "            subfolder=\"tokenizer\",\n",
    "            revision=args.revision,\n",
    "            use_fast=False,\n",
    "        )\n",
    "\n",
    "    text_encoder_cls = import_model_class_from_model_name_or_path(args.pretrained_model_name_or_path, args.revision)\n",
    "\n",
    "    noise_scheduler = DDPMScheduler.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"scheduler\")\n",
    "    text_encoder = text_encoder_cls.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"text_encoder\", revision=args.revision\n",
    "    )\n",
    "    vae = AutoencoderKL.from_pretrained(args.pretrained_model_name_or_path, subfolder=\"vae\", revision=args.revision)\n",
    "    unet = UNet2DConditionModel.from_pretrained(\n",
    "        args.pretrained_model_name_or_path, subfolder=\"unet\", revision=args.revision\n",
    "    )\n",
    "\n",
    "    vae.requires_grad_(False)\n",
    "    text_encoder.requires_grad_(False)\n",
    "    unet.requires_grad_(False)\n",
    "\n",
    "    weight_dtype = torch.float32\n",
    "    if accelerator.mixed_precision == \"fp16\":\n",
    "        weight_dtype = torch.float16\n",
    "    elif accelerator.mixed_precision == \"bf16\":\n",
    "        weight_dtype = torch.bfloat16\n",
    "\n",
    "    unet.to(accelerator.device, dtype=weight_dtype)\n",
    "    vae.to(accelerator.device, dtype=weight_dtype)\n",
    "    text_encoder.to(accelerator.device, dtype=weight_dtype)\n",
    "\n",
    "    if args.enable_xformers_memory_efficient_attention:\n",
    "        if is_xformers_available():\n",
    "            unet.enable_xformers_memory_efficient_attention()\n",
    "        else:\n",
    "            raise ValueError(\"xformers is not available. Make sure it is installed correctly\")\n",
    "\n",
    "    lora_attn_procs = {}\n",
    "    for name in unet.attn_processors.keys():\n",
    "        cross_attention_dim = None if name.endswith(\"attn1.processor\") else unet.config.cross_attention_dim\n",
    "        if name.startswith(\"mid_block\"):\n",
    "            hidden_size = unet.config.block_out_channels[-1]\n",
    "        elif name.startswith(\"up_blocks\"):\n",
    "            block_id = int(name[len(\"up_blocks.\")])\n",
    "            hidden_size = list(reversed(unet.config.block_out_channels))[block_id]\n",
    "        elif name.startswith(\"down_blocks\"):\n",
    "            block_id = int(name[len(\"down_blocks.\")])\n",
    "            hidden_size = unet.config.block_out_channels[block_id]\n",
    "\n",
    "        lora_attn_procs[name] = LoRACrossAttnProcessor(\n",
    "            hidden_size=hidden_size, cross_attention_dim=cross_attention_dim\n",
    "        )\n",
    "\n",
    "    unet.set_attn_processor(lora_attn_procs)\n",
    "    lora_layers = AttnProcsLayers(unet.attn_processors)\n",
    "\n",
    "    accelerator.register_for_checkpointing(lora_layers)\n",
    "\n",
    "    if args.scale_lr:\n",
    "        args.learning_rate = (\n",
    "            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    if args.allow_tf32:\n",
    "        torch.backends.cuda.matmul.allow_tf32 = True\n",
    "\n",
    "    if args.scale_lr:\n",
    "        args.learning_rate = (\n",
    "            args.learning_rate * args.gradient_accumulation_steps * args.train_batch_size * accelerator.num_processes\n",
    "        )\n",
    "\n",
    "    if args.use_8bit_adam:\n",
    "        try:\n",
    "            import bitsandbytes as bnb\n",
    "        except ImportError:\n",
    "            raise ImportError(\n",
    "                \"To use 8-bit Adam, please install the bitsandbytes library: `pip install bitsandbytes`.\"\n",
    "            )\n",
    "\n",
    "        optimizer_class = bnb.optim.AdamW8bit\n",
    "    else:\n",
    "        optimizer_class = torch.optim.AdamW\n",
    "\n",
    "    optimizer = optimizer_class(\n",
    "        lora_layers.parameters(),\n",
    "        lr=args.learning_rate,\n",
    "        betas=(args.adam_beta1, args.adam_beta2),\n",
    "        weight_decay=args.adam_weight_decay,\n",
    "        eps=args.adam_epsilon,\n",
    "    )\n",
    "\n",
    "    train_dataset = IdilDataSet(\n",
    "        csv_path=args.instance_data_dir,\n",
    "        folder=args.class_data_dir if args.with_prior_preservation else args.instance_data_dir,\n",
    "        magnification=20,\n",
    "        transform=get_transforms(train=True),\n",
    "        n_patches=250,\n",
    "        random_selection=True,\n",
    "        limit=None,\n",
    "        wsi_type=\"frozen\"\n",
    "    )\n",
    "\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        batch_size=args.train_batch_size,\n",
    "        shuffle=True,\n",
    "        collate_fn=lambda examples: collate_fn(examples, args.with_prior_preservation),\n",
    "        num_workers=args.dataloader_num_workers,\n",
    "    )\n",
    "\n",
    "    overrode_max_train_steps = False\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if args.max_train_steps is None:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "        overrode_max_train_steps = True\n",
    "\n",
    "        lr_scheduler = get_scheduler(\n",
    "        args.lr_scheduler,\n",
    "        optimizer=optimizer,\n",
    "        num_warmup_steps=args.lr_warmup_steps * args.gradient_accumulation_steps,\n",
    "        num_training_steps=args.max_train_steps * args.gradient_accumulation_steps,\n",
    "        num_cycles=args.lr_num_cycles,\n",
    "        power=args.lr_power,\n",
    "    )\n",
    "\n",
    "    lora_layers, optimizer, train_dataloader, lr_scheduler = accelerator.prepare(\n",
    "        lora_layers, optimizer, train_dataloader, lr_scheduler\n",
    "    )\n",
    "\n",
    "    num_update_steps_per_epoch = math.ceil(len(train_dataloader) / args.gradient_accumulation_steps)\n",
    "    if overrode_max_train_steps:\n",
    "        args.max_train_steps = args.num_train_epochs * num_update_steps_per_epoch\n",
    "    args.num_train_epochs = math.ceil(args.max_train_steps / num_update_steps_per_epoch)\n",
    "\n",
    "    if accelerator.is_main_process:\n",
    "        accelerator.init_trackers(\"dreambooth-lora\", config=vars(args))\n",
    "\n",
    "    total_batch_size = args.train_batch_size * accelerator.num_processes * args.gradient_accumulation_steps\n",
    "\n",
    "    logger.info(\"***** Running training *****\")\n",
    "    logger.info(f\"  Num examples = {len(train_dataset)}\")\n",
    "    logger.info(f\"  Num batches each epoch = {len(train_dataloader)}\")\n",
    "    logger.info(f\"  Num Epochs = {args.num_train_epochs}\")\n",
    "    logger.info(f\"  Instantaneous batch size per device = {args.train_batch_size}\")\n",
    "    logger.info(f\"  Total train batch size (w. parallel, distributed & accumulation) = {total_batch_size}\")\n",
    "    logger.info(f\"  Gradient Accumulation steps = {args.gradient_accumulation_steps}\")\n",
    "    logger.info(f\"  Total optimization steps = {args.max_train_steps}\")\n",
    "    global_step = 0\n",
    "    first_epoch = 0\n",
    "\n",
    "    if args.resume_from_checkpoint:\n",
    "        if args.resume_from_checkpoint != \"latest\":\n",
    "            path = os.path.basename(args.resume_from_checkpoint)\n",
    "        else:\n",
    "            dirs = os.listdir(args.output_dir)\n",
    "            dirs = [d for d in dirs if d.startswith(\"checkpoint\")]\n",
    "            dirs = sorted(dirs, key=lambda x: int(x.split(\"-\")[1]))\n",
    "            path = dirs[-1] if len(dirs) > 0 else None\n",
    "\n",
    "        if path is None:\n",
    "            accelerator.print(\n",
    "                f\"Checkpoint '{args.resume_from_checkpoint}' does not exist. Starting a new training run.\"\n",
    "            )\n",
    "            args.resume_from_checkpoint = None\n",
    "        else:\n",
    "            accelerator.print(f\"Resuming from checkpoint {path}\")\n",
    "            accelerator.load_state(os.path.join(args.output_dir, path))\n",
    "            global_step = int(path.split(\"-\")[1])\n",
    "\n",
    "            resume_global_step = global_step * args.gradient_accumulation_steps\n",
    "            first_epoch = global_step // num_update_steps_per_epoch\n",
    "            resume_step = resume_global_step % (num_update_steps_per_epoch * args.gradient_accumulation_steps)\n",
    "\n",
    "    progress_bar = tqdm(range(global_step, args.max_train_steps), disable=not accelerator.is_local_main_process)\n",
    "    progress_bar.set_description(\"Steps\")\n",
    "\n",
    "    for epoch in range(first_epoch, args.num_train_epochs):\n",
    "        unet.train()\n",
    "        for step, batch in enumerate(train_dataloader):\n",
    "            if args.resume_from_checkpoint and epoch == first_epoch and step < resume_step:\n",
    "                if step % args.gradient_accumulation_steps == 0:\n",
    "                    progress_bar.update(1)\n",
    "                continue\n",
    "\n",
    "            with accelerator.accumulate(unet):\n",
    "                latents = vae.encode(batch[\"pixel_values\"].to(dtype=weight_dtype)).latent_dist.sample()\n",
    "                latents = latents * 0.18215\n",
    "\n",
    "                noise = torch.randn_like(latents)\n",
    "                bsz = latents.shape[0]\n",
    "                timesteps = torch.randint(0, noise_scheduler.config.num_train_timesteps, (bsz,), device=latents.device)\n",
    "                timesteps = timesteps.long()\n",
    "\n",
    "                noisy_latents = noise_scheduler.add_noise(latents, noise, timesteps)\n",
    "\n",
    "                encoder_hidden_states = text_encoder(batch[\"input_ids\"])[0]\n",
    "\n",
    "                model_pred = unet(noisy_latents, timesteps, encoder_hidden_states).sample\n",
    "\n",
    "                if noise_scheduler.config.prediction_type == \"epsilon\":\n",
    "                    target = noise\n",
    "                elif noise_scheduler.config.prediction_type == \"v_prediction\":\n",
    "                    target = noise_scheduler.get_velocity(latents, noise, timesteps)\n",
    "                else:\n",
    "                    raise ValueError(f\"Unknown prediction type {noise_scheduler.config.prediction_type}\")\n",
    "\n",
    "                if args.with_prior_preservation:\n",
    "                    model_pred, model_pred_prior = torch.chunk(model_pred, 2, dim=0)\n",
    "                    target, target_prior = torch.chunk(target, 2, dim=0)\n",
    "\n",
    "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "                    prior_loss = F.mse_loss(model_pred_prior.float(), target_prior.float(), reduction=\"mean\")\n",
    "\n",
    "                    loss = loss + args.prior_loss_weight * prior_loss\n",
    "                else:\n",
    "                    loss = F.mse_loss(model_pred.float(), target.float(), reduction=\"mean\")\n",
    "\n",
    "                accelerator.backward(loss)\n",
    "                if accelerator.sync_gradients:\n",
    "                    params_to_clip = lora_layers.parameters()\n",
    "                    accelerator.clip_grad_norm_(params_to_clip, args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                lr_scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "            if accelerator.sync_gradients:\n",
    "                progress_bar.update(1)\n",
    "                global_step += 1\n",
    "\n",
    "                if global_step % args.checkpointing_steps == 0:\n",
    "                    if accelerator.is_main_process:\n",
    "                        save_path = os.path.join(args.output_dir, f\"checkpoint-{global_step}\")\n",
    "                        accelerator.save_state(save_path)\n",
    "                        logger.info(f\"Saved state to {save_path}\")\n",
    "\n",
    "            logs = {\"loss\": loss.detach().item(), \"lr\": lr_scheduler.get_last_lr()[0]}\n",
    "            progress_bar.set_postfix(**logs)\n",
    "            accelerator.log(logs, step=global_step)\n",
    "\n",
    "            if global_step >= args.max_train_steps:\n",
    "                break\n",
    "\n",
    "        if args.validation_prompt is not None and epoch % args.validation_epochs == 0:\n",
    "            logger.info(\n",
    "                f\"Running validation... \\n Generating {args.num_validation_images} images with prompt:\"\n",
    "                f\" {args.validation_prompt}.\"\n",
    "            )\n",
    "            pipeline = DiffusionPipeline.from_pretrained(\n",
    "                args.pretrained_model_name_or_path,\n",
    "                unet=accelerator.unwrap_model(unet),\n",
    "                text_encoder=accelerator.unwrap_model(text_encoder),\n",
    "                revision=args.revision,\n",
    "                torch_dtype=weight_dtype,\n",
    "            )\n",
    "            pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "            pipeline = pipeline.to(accelerator.device)\n",
    "            pipeline.set_progress_bar_config(disable=True)\n",
    "\n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(args.seed)\n",
    "            prompt = args.num_validation_images * [args.validation_prompt]\n",
    "            images = pipeline(prompt, num_inference_steps=25, generator=generator).images\n",
    "\n",
    "            for tracker in accelerator.trackers:\n",
    "                if tracker.name == \"tensorboard\":\n",
    "                    np_images = np.stack([np.asarray(img) for img in images])\n",
    "                    tracker.writer.add_images(\"validation\", np_images, epoch, dataformats=\"NHWC\")\n",
    "                if tracker.name == \"wandb\":\n",
    "                    tracker.log(\n",
    "                        {\n",
    "                            \"validation\": [\n",
    "                                wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\n",
    "                                for i, image in enumerate(images)\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "            del pipeline\n",
    "            torch.cuda.empty_cache()\n",
    "\n",
    "    accelerator.wait_for_everyone()\n",
    "    if accelerator.is_main_process:\n",
    "        unet = unet.to(torch.float32)\n",
    "        unet.save_attn_procs(args.output_dir)\n",
    "\n",
    "        pipeline = DiffusionPipeline.from_pretrained(\n",
    "            args.pretrained_model_name_or_path, revision=args.revision, torch_dtype=weight_dtype\n",
    "        )\n",
    "        pipeline.scheduler = DPMSolverMultistepScheduler.from_config(pipeline.scheduler.config)\n",
    "        pipeline = pipeline.to(accelerator.device)\n",
    "\n",
    "        pipeline.unet.load_attn_procs(args.output_dir)\n",
    "\n",
    "        if args.validation_prompt and args.num_validation_images > 0:\n",
    "            generator = torch.Generator(device=accelerator.device).manual_seed(args.seed) if args.seed else None\n",
    "            prompt = args.num_validation_images * [args.validation_prompt]\n",
    "            images = pipeline(prompt, num_inference_steps=25, generator=generator).images\n",
    "\n",
    "            test_image_dir = Path(args.output_dir) / 'test_images'\n",
    "            test_image_dir.mkdir()\n",
    "            for i, image in enumerate(images):\n",
    "                out_path = test_image_dir / f'image_{i}.png'\n",
    "                image.save(out_path)\n",
    "\n",
    "            for tracker in accelerator.trackers:\n",
    "                if tracker.name == \"tensorboard\":\n",
    "                    np_images = np.stack([np.asarray(img) for img in images])\n",
    "                    tracker.writer.add_images(\"test\", np_images, epoch, dataformats=\"NHWC\")\n",
    "                if tracker.name == \"wandb\":\n",
    "                    tracker.log(\n",
    "                        {\n",
    "                            \"test\": [\n",
    "                                wandb.Image(image, caption=f\"{i}: {args.validation_prompt}\")\n",
    "                                for i, image in enumerate(images)\n",
    "                            ]\n",
    "                        }\n",
    "                    )\n",
    "\n",
    "        if args.push_to_hub:\n",
    "            save_model_card(\n",
    "                repo_name,\n",
    "                images=images,\n",
    "                base_model=args.pretrained_model_name_or_path,\n",
    "                prompt=args.instance_prompt,\n",
    "                repo_folder=args.output_dir,\n",
    "            )\n",
    "            repo.push_to_hub(commit_message=\"End of training\", blocking=False, auto_lfs_prune=True)\n",
    "\n",
    "    accelerator.end_training()\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    main(args)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1cdcdf2-6775-43ef-9678-ee63054d712b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
